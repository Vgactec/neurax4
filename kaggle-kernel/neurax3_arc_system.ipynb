{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction-section",
   "metadata": {},
   "source": [
    "# Système Neurax3 Optimisé pour ARC-Prize-2025\n",
    "\n",
    "Version optimisée avec toutes les recommandations appliquées :\n",
    "- Traitement parallèle GPU optimisé\n",
    "- Gestion mémoire améliorée\n",
    "- Points de reprise robustes\n",
    "- Surveillance temps réel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-section",
   "metadata": {},
   "source": [
    "# Configuration initiale optimisée\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configuration des dossiers\n",
    "for folder in ['logs', 'reports', 'checkpoints', 'outputs']:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"logs/neurax3.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configuration de l'environnement\n",
    "np.random.seed(2025)\n",
    "torch.manual_seed(2025)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(2025)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"ATTENTION: GPU non disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantum-gravity-simulator",
   "metadata": {},
   "source": [
    "class QuantumGravitySimulator:\n",
    "    \"\"\"Simulateur de gravité quantique pour les puzzles ARC\"\"\"\n",
    "    \n",
    "    def __init__(self, use_gpu=True, precision=\"float32\", memory_efficient=True):\n",
    "        self.use_gpu = use_gpu and torch.cuda.is_available()\n",
    "        self.precision = precision\n",
    "        self.memory_efficient = memory_efficient\n",
    "        self.device = torch.device(\"cuda\" if self.use_gpu else \"cpu\")\n",
    "        self.dtype = torch.float32 if precision == \"float32\" else torch.float16\n",
    "        \n",
    "        # Initialisation des paramètres quantiques\n",
    "        self.quantum_fields = {\n",
    "            \"primary\": None,\n",
    "            \"secondary\": None,\n",
    "            \"tertiary\": None,\n",
    "            \"quaternary\": None\n",
    "        }\n",
    "        self.interaction_strengths = self._initialize_interaction_strengths()\n",
    "        self.relativistic_effects = self._initialize_relativistic_effects()\n",
    "        \n",
    "        # Configurations avancées pour le GPU\n",
    "        if self.use_gpu:\n",
    "            # Activer la précision mixte pour économiser la mémoire GPU\n",
    "            self.scaler = torch.cuda.amp.GradScaler(enabled=(precision == \"float16\"))\n",
    "            # Optimiser la mémoire cache L2\n",
    "            torch.cuda.set_per_process_memory_fraction(0.95)\n",
    "            # Activer les tensor cores pour les opérations matricielles\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    def _initialize_interaction_strengths(self):\n",
    "        \"\"\"Initialise les forces d'interaction entre les champs quantiques\"\"\"\n",
    "        return {\n",
    "            \"gravitational\": 1.0,\n",
    "            \"electromagnetic\": 0.7,\n",
    "            \"strong_nuclear\": 0.3,\n",
    "            \"weak_nuclear\": 0.1,\n",
    "            \"quantum_entanglement\": 0.5\n",
    "        }\n",
    "    \n",
    "    def _initialize_relativistic_effects(self):\n",
    "        \"\"\"Initialise les effets relativistes dans la simulation\"\"\"\n",
    "        return {\n",
    "            \"time_dilation\": 0.2,\n",
    "            \"length_contraction\": 0.1,\n",
    "            \"mass_energy_equivalence\": 0.3,\n",
    "            \"quantum_tunneling\": 0.4\n",
    "        }\n",
    "    \n",
    "    def initialize_fields(self, grid_size):\n",
    "        \"\"\"Initialise les champs quantiques pour une taille de grille donnée\"\"\"\n",
    "        # Création des champs quantiques sur le dispositif approprié\n",
    "        for field_name in self.quantum_fields:\n",
    "            self.quantum_fields[field_name] = torch.zeros(\n",
    "                (grid_size, grid_size), \n",
    "                dtype=self.dtype, \n",
    "                device=self.device\n",
    "            )\n",
    "    \n",
    "    def apply_quantum_gravity(self, grid, iterations=5):\n",
    "        \"\"\"Applique les effets de gravité quantique sur une grille\"\"\"\n",
    "        # Conversion de la grille en tenseur PyTorch\n",
    "        if not isinstance(grid, torch.Tensor):\n",
    "            grid = torch.tensor(grid, dtype=self.dtype, device=self.device)\n",
    "        elif grid.device != self.device or grid.dtype != self.dtype:\n",
    "            grid = grid.to(device=self.device, dtype=self.dtype)\n",
    "            \n",
    "        # Initialisation des champs si nécessaire\n",
    "        if self.quantum_fields[\"primary\"] is None or self.quantum_fields[\"primary\"].shape != grid.shape:\n",
    "            self.initialize_fields(grid.shape[0])\n",
    "        \n",
    "        # Application des effets quantiques gravitationnels\n",
    "        with torch.cuda.amp.autocast(enabled=(self.precision == \"float16\")):\n",
    "            # Mise à jour du champ primaire\n",
    "            self.quantum_fields[\"primary\"] = grid.clone()\n",
    "            \n",
    "            # Calcul des champs secondaires\n",
    "            for i in range(iterations):\n",
    "                # Interactions gravitationnelles\n",
    "                gravitational_field = self._compute_gravitational_field(self.quantum_fields[\"primary\"])\n",
    "                self.quantum_fields[\"secondary\"] = gravitational_field\n",
    "                \n",
    "                # Interactions électromagnétiques\n",
    "                electromagnetic_field = self._compute_electromagnetic_field(self.quantum_fields[\"primary\"])\n",
    "                self.quantum_fields[\"tertiary\"] = electromagnetic_field\n",
    "                \n",
    "                # Effets quantiques et relativistes\n",
    "                quantum_field = self._apply_quantum_effects(self.quantum_fields[\"primary\"], \n",
    "                                                        self.quantum_fields[\"secondary\"],\n",
    "                                                        self.quantum_fields[\"tertiary\"])\n",
    "                self.quantum_fields[\"quaternary\"] = quantum_field\n",
    "                \n",
    "                # Combinaison des champs pour l'itération suivante\n",
    "                combined_field = self._combine_fields()\n",
    "                self.quantum_fields[\"primary\"] = combined_field\n",
    "        \n",
    "        # Conversion finale et nettoyage de la mémoire si nécessaire\n",
    "        result = self.quantum_fields[\"primary\"].clone()\n",
    "        if self.memory_efficient and self.use_gpu:\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def _compute_gravitational_field(self, field):\n",
    "        \"\"\"Calcule le champ gravitationnel à partir d'un champ donné\"\"\"\n",
    "        # Convolution pour simuler les effets gravitationnels\n",
    "        kernel = torch.tensor([[0.5, 1.0, 0.5],\n",
    "                              [1.0, 0.0, 1.0],\n",
    "                              [0.5, 1.0, 0.5]], dtype=self.dtype, device=self.device)\n",
    "        kernel = kernel.view(1, 1, 3, 3)\n",
    "        \n",
    "        # Préparer le champ pour la convolution\n",
    "        field_reshaped = field.view(1, 1, field.shape[0], field.shape[1])\n",
    "        \n",
    "        # Appliquer la convolution\n",
    "        padding = 1\n",
    "        gravitational_field = torch.nn.functional.conv2d(field_reshaped, kernel, padding=padding)\n",
    "        \n",
    "        # Normaliser et appliquer la force gravitationnelle\n",
    "        gravitational_field = gravitational_field.view(field.shape)\n",
    "        gravitational_field = gravitational_field * self.interaction_strengths[\"gravitational\"]\n",
    "        \n",
    "        return gravitational_field\n",
    "    \n",
    "    def _compute_electromagnetic_field(self, field):\n",
    "        \"\"\"Calcule le champ électromagnétique à partir d'un champ donné\"\"\"\n",
    "        # Détecter les gradients pour simuler les effets électromagnétiques\n",
    "        sobel_x = torch.tensor([[-1, 0, 1],\n",
    "                              [-2, 0, 2],\n",
    "                              [-1, 0, 1]], dtype=self.dtype, device=self.device)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1],\n",
    "                              [0, 0, 0],\n",
    "                              [1, 2, 1]], dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        sobel_x = sobel_x.view(1, 1, 3, 3)\n",
    "        sobel_y = sobel_y.view(1, 1, 3, 3)\n",
    "        \n",
    "        # Préparer le champ pour la convolution\n",
    "        field_reshaped = field.view(1, 1, field.shape[0], field.shape[1])\n",
    "        \n",
    "        # Appliquer les filtres Sobel\n",
    "        padding = 1\n",
    "        grad_x = torch.nn.functional.conv2d(field_reshaped, sobel_x, padding=padding)\n",
    "        grad_y = torch.nn.functional.conv2d(field_reshaped, sobel_y, padding=padding)\n",
    "        \n",
    "        # Calculer la magnitude du gradient\n",
    "        electromagnetic_field = torch.sqrt(grad_x**2 + grad_y**2 + 1e-6)\n",
    "        electromagnetic_field = electromagnetic_field.view(field.shape)\n",
    "        \n",
    "        # Appliquer la force électromagnétique\n",
    "        electromagnetic_field = electromagnetic_field * self.interaction_strengths[\"electromagnetic\"]\n",
    "        \n",
    "        return electromagnetic_field\n",
    "    \n",
    "    def _apply_quantum_effects(self, primary_field, secondary_field, tertiary_field):\n",
    "        \"\"\"Applique les effets quantiques sur les champs\"\"\"\n",
    "        # Entanglement quantique entre les champs\n",
    "        entanglement = (primary_field * secondary_field * self.interaction_strengths[\"quantum_entanglement\"])\n",
    "        \n",
    "        # Effets tunnels quantiques\n",
    "        tunneling = torch.sigmoid(tertiary_field) * self.relativistic_effects[\"quantum_tunneling\"]\n",
    "        \n",
    "        # Combinaison des effets quantiques\n",
    "        quantum_field = primary_field + entanglement + tunneling\n",
    "        \n",
    "        return quantum_field\n",
    "    \n",
    "    def _combine_fields(self):\n",
    "        \"\"\"Combine tous les champs quantiques pour produire le champ résultant\"\"\"\n",
    "        # Moyenne pondérée des champs\n",
    "        weights = torch.tensor([0.4, 0.3, 0.2, 0.1], dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        combined_field = (\n",
    "            weights[0] * self.quantum_fields[\"primary\"] +\n",
    "            weights[1] * self.quantum_fields[\"secondary\"] +\n",
    "            weights[2] * self.quantum_fields[\"tertiary\"] +\n",
    "            weights[3] * self.quantum_fields[\"quaternary\"]\n",
    "        )\n",
    "        \n",
    "        # Normalisation des valeurs\n",
    "        combined_field = torch.clamp(combined_field, 0, 10)\n",
    "        \n",
    "        return combined_field\n",
    "    \n",
    "    def process_grid(self, grid, time_steps=5):\n",
    "        \"\"\"Traite une grille de puzzle ARC avec des effets gravitationnels quantiques\"\"\"\n",
    "        # Conversion initiale de la grille\n",
    "        if not isinstance(grid, torch.Tensor):\n",
    "            processed_grid = torch.tensor(grid, dtype=self.dtype, device=self.device)\n",
    "        else:\n",
    "            processed_grid = grid.clone().to(device=self.device, dtype=self.dtype)\n",
    "            \n",
    "        # Application de la gravité quantique pour chaque étape temporelle\n",
    "        for _ in range(time_steps):\n",
    "            processed_grid = self.apply_quantum_gravity(processed_grid)\n",
    "            \n",
    "        # Conversion finale vers le format entier pour ARC\n",
    "        result = torch.round(processed_grid).to(torch.int64)\n",
    "        \n",
    "        # Conversion en numpy pour compatibilité avec ARC\n",
    "        if self.use_gpu:\n",
    "            result = result.cpu().numpy()\n",
    "        else:\n",
    "            result = result.numpy()\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-quantum-simulator",
   "metadata": {},
   "source": [
    "def enhance_quantum_gravity_simulator(engine):\n",
    "    \"\"\"Implémente les extensions physiques avancées pour le simulateur de gravité quantique.\"\"\"\n",
    "    # Ajouter les champs quantiques supplémentaires\n",
    "    engine.quantum_fields[\"quintessence\"] = None\n",
    "    engine.quantum_fields[\"dark_matter\"] = None\n",
    "    engine.quantum_fields[\"dark_energy\"] = None\n",
    "    \n",
    "    # Augmenter les forces d'interaction\n",
    "    engine.interaction_strengths[\"quantum_entanglement\"] = 0.75  # Augmenter l'intrication quantique\n",
    "    engine.interaction_strengths[\"non_local\"] = 0.6  # Ajouter les interactions non-locales\n",
    "    engine.interaction_strengths[\"dimensional\"] = 0.4  # Ajouter les interactions dimensionnelles\n",
    "    \n",
    "    # Améliorer les effets relativistes\n",
    "    engine.relativistic_effects[\"time_dilation\"] = 0.3  # Augmenter les effets de dilatation temporelle\n",
    "    engine.relativistic_effects[\"quantum_tunneling\"] = 0.6  # Augmenter les effets de tunnelisation\n",
    "    engine.relativistic_effects[\"superposition\"] = 0.5  # Ajouter la superposition quantique\n",
    "    \n",
    "    # Extension pour les algorithmes adaptatifs\n",
    "    original_initialize_fields = engine.initialize_fields\n",
    "    \n",
    "    def enhanced_initialize_fields(grid_size):\n",
    "        # Appel à la méthode originale\n",
    "        original_initialize_fields(grid_size)\n",
    "        \n",
    "        # Initialisation des champs supplémentaires\n",
    "        engine.quantum_fields[\"quintessence\"] = torch.zeros(\n",
    "            (grid_size, grid_size), \n",
    "            dtype=engine.dtype, \n",
    "            device=engine.device\n",
    "        )\n",
    "        engine.quantum_fields[\"dark_matter\"] = torch.zeros(\n",
    "            (grid_size, grid_size), \n",
    "            dtype=engine.dtype, \n",
    "            device=engine.device\n",
    "        )\n",
    "        engine.quantum_fields[\"dark_energy\"] = torch.zeros(\n",
    "            (grid_size, grid_size), \n",
    "            dtype=engine.dtype, \n",
    "            device=engine.device\n",
    "        )\n",
    "    \n",
    "    # Remplacer la méthode d'initialisation\n",
    "    engine.initialize_fields = enhanced_initialize_fields\n",
    "    \n",
    "    # Extension pour la compression des états quantiques\n",
    "    original_combine_fields = engine._combine_fields\n",
    "    \n",
    "    def enhanced_combine_fields():\n",
    "        # Obtenir le champ combiné de base\n",
    "        combined_field = original_combine_fields()\n",
    "        \n",
    "        # Ajouter l'influence des champs supplémentaires s'ils existent\n",
    "        if engine.quantum_fields[\"quintessence\"] is not None:\n",
    "            # Effet de quintessence (influencé par les motifs globaux)\n",
    "            quintessence_weight = 0.15\n",
    "            combined_field = combined_field + quintessence_weight * engine.quantum_fields[\"quintessence\"]\n",
    "        \n",
    "        if engine.quantum_fields[\"dark_matter\"] is not None:\n",
    "            # Effet de matière noire (augmente la \"masse\" des motifs)\n",
    "            dark_matter_weight = 0.2\n",
    "            combined_field = combined_field + dark_matter_weight * engine.quantum_fields[\"dark_matter\"]\n",
    "            \n",
    "        if engine.quantum_fields[\"dark_energy\"] is not None:\n",
    "            # Effet d'énergie noire (expansion des motifs)\n",
    "            dark_energy_weight = 0.1\n",
    "            combined_field = combined_field + dark_energy_weight * engine.quantum_fields[\"dark_energy\"]\n",
    "        \n",
    "        # Compression des états quantiques (réduire la dispersion des valeurs)\n",
    "        mean = torch.mean(combined_field)\n",
    "        std = torch.std(combined_field) + 1e-6\n",
    "        normalized_field = (combined_field - mean) / std\n",
    "        compressed_field = torch.tanh(normalized_field) * 5 + 5  # Remappe dans la plage 0-10\n",
    "        \n",
    "        # Fusion contrôlée entre le champ original et le champ compressé\n",
    "        compression_ratio = 0.3\n",
    "        final_field = (1 - compression_ratio) * combined_field + compression_ratio * compressed_field\n",
    "        \n",
    "        # Garantir que les valeurs restent dans la plage attendue\n",
    "        final_field = torch.clamp(final_field, 0, 10)\n",
    "        \n",
    "        return final_field\n",
    "    \n",
    "    # Remplacer la méthode de combinaison\n",
    "    engine._combine_fields = enhanced_combine_fields\n",
    "    \n",
    "    return engine  # Retourner le moteur amélioré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configure-gpu-section",
   "metadata": {},
   "source": [
    "def configure_engine_for_gpu(engine):\n",
    "    \"\"\"Configure le moteur Neurax pour utiliser le GPU de manière optimale\"\"\"\n",
    "    # Vérifier si un GPU est disponible\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"ATTENTION: Aucun GPU détecté. Utilisation du CPU.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Configurer pour utiliser le GPU avec des options avancées\n",
    "        print(f\"Configuration GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        # Optimisations CUDA\n",
    "        torch.backends.cudnn.benchmark = True  # Optimiser les performances pour des tailles d'entrée constantes\n",
    "        torch.backends.cudnn.deterministic = False  # Permettre des optimisations non-déterministes\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True  # Utiliser les Tensor Cores si disponibles\n",
    "        \n",
    "        # Optimiser l'allocation mémoire\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Vérifier la mémoire GPU disponible\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "        gpu_memory_allocated = torch.cuda.memory_allocated(0)\n",
    "        gpu_memory_free = gpu_memory - gpu_memory_allocated\n",
    "        \n",
    "        print(f\"Mémoire GPU totale: {gpu_memory/1024**3:.2f} GB\")\n",
    "        print(f\"Mémoire GPU disponible: {gpu_memory_free/1024**3:.2f} GB\")\n",
    "        \n",
    "        # Configurer la précision selon la mémoire disponible\n",
    "        if gpu_memory_free > 8 * 1024**3:\n",
    "            print(\"Utilisation de la précision float32 pour une meilleure précision\")\n",
    "            engine.precision = \"float32\"\n",
    "            engine.dtype = torch.float32\n",
    "        else:\n",
    "            print(\"Utilisation de la précision mixte (float16) pour économiser la mémoire\")\n",
    "            engine.precision = \"float16\"\n",
    "            engine.dtype = torch.float16\n",
    "            engine.scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "        \n",
    "        # Vérifier si les champs quantiques sont déjà sur GPU\n",
    "        for field_name, field in engine.quantum_fields.items():\n",
    "            if field is not None and field.device.type != \"cuda\":\n",
    "                engine.quantum_fields[field_name] = field.to(device=\"cuda\", dtype=engine.dtype)\n",
    "        \n",
    "        # Nettoyer la mémoire GPU\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERREUR lors de la configuration du GPU: {str(e)}\")\n",
    "        print(\"Retour au mode CPU\")\n",
    "        engine.use_gpu = False\n",
    "        engine.device = torch.device(\"cpu\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-management-section",
   "metadata": {},
   "source": [
    "def save_checkpoint(processed_ids, phase):\n",
    "    \"\"\"Sauvegarde un point de reprise pour le traitement des puzzles\"\"\"\n",
    "    checkpoint_path = f\"checkpoints/{phase}_checkpoint.json\"\n",
    "    with open(checkpoint_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'processed_ids': processed_ids,\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, f)\n",
    "    print(f\"Point de reprise sauvegardé: {len(processed_ids)} puzzles traités\")\n",
    "    \n",
    "def load_checkpoint(phase):\n",
    "    \"\"\"Charge un point de reprise existant\"\"\"\n",
    "    checkpoint_path = f\"checkpoints/{phase}_checkpoint.json\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "        print(f\"Point de reprise chargé: {len(checkpoint['processed_ids'])} puzzles déjà traités\")\n",
    "        return checkpoint['processed_ids']\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-puzzles-section",
   "metadata": {},
   "source": [
    "def process_puzzles_optimized(puzzles, engine, max_time_per_puzzle=None, phase=\"test\", verify_solutions=False):\n",
    "    \"\"\"Version optimisée de process_puzzles sans aucune limitation.\"\"\"\n",
    "    results = []\n",
    "    total_puzzles = len(puzzles)\n",
    "    processed_ids = load_checkpoint(phase)\n",
    "    processed_set = set(processed_ids)\n",
    "    \n",
    "    # Statistiques\n",
    "    total_time = 0\n",
    "    successful_puzzles = 0\n",
    "    skipped_puzzles = 0\n",
    "    \n",
    "    # Logging\n",
    "    log_path = f\"logs/{phase}_processing.log\"\n",
    "    with open(log_path, 'a') as log_file:\n",
    "        log_file.write(f\"\\n=== Début du traitement {phase} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\\n\")\n",
    "        log_file.write(f\"Total de puzzles à traiter: {total_puzzles}\\n\")\n",
    "        log_file.write(f\"Puzzles déjà traités: {len(processed_ids)}\\n\")\n",
    "    \n",
    "    print(f\"Traitement de {total_puzzles} puzzles ({phase})...\")\n",
    "    \n",
    "    # AUCUNE limitation sur le nombre de puzzles - traitement absolument complet\n",
    "    for i, puzzle in enumerate(tqdm(puzzles, desc=f\"Traitement {phase}\")):\n",
    "        puzzle_id = puzzle.get('id', f\"{phase}_{i}\")\n",
    "        \n",
    "        # Sauter les puzzles déjà traités\n",
    "        if puzzle_id in processed_set:\n",
    "            skipped_puzzles += 1\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Conversion du format de puzzle\n",
    "            converted_puzzle = convert_puzzle_format(puzzle)\n",
    "            \n",
    "            # Traitement du puzzle avec le moteur quantique\n",
    "            input_grid = np.array(converted_puzzle['input'])\n",
    "            processed_grid = engine.process_grid(input_grid, time_steps=8)\n",
    "            \n",
    "            # Assurer que la sortie est de type int\n",
    "            output_grid = processed_grid.astype(int).tolist()\n",
    "            \n",
    "            # Calculer le temps de traitement\n",
    "            processing_time = time.time() - start_time\n",
    "            total_time += processing_time\n",
    "            \n",
    "            # Vérification des solutions (si demandé et disponible)\n",
    "            solution_correct = None\n",
    "            if verify_solutions and 'output' in converted_puzzle:\n",
    "                expected_output = np.array(converted_puzzle['output'])\n",
    "                predicted_output = np.array(output_grid)\n",
    "                solution_correct = np.array_equal(expected_output, predicted_output)\n",
    "            \n",
    "            # Enregistrer le résultat\n",
    "            result = {\n",
    "                'id': puzzle_id,\n",
    "                'output': output_grid,\n",
    "                'processing_time': processing_time,\n",
    "                'solution_correct': solution_correct\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Mettre à jour les statistiques\n",
    "            successful_puzzles += 1\n",
    "            \n",
    "            # Ajouter à la liste des puzzles traités\n",
    "            processed_ids.append(puzzle_id)\n",
    "            processed_set.add(puzzle_id)\n",
    "            \n",
    "            # Logging détaillé\n",
    "            with open(log_path, 'a') as log_file:\n",
    "                log_file.write(f\"Puzzle {puzzle_id} traité en {processing_time:.2f}s\")\n",
    "                if solution_correct is not None:\n",
    "                    log_file.write(f\" - Solution correcte: {solution_correct}\")\n",
    "                log_file.write(\"\\n\")\n",
    "            \n",
    "            # Sauvegarde périodique des points de reprise (tous les 10 puzzles)\n",
    "            if len(results) % 10 == 0:\n",
    "                save_checkpoint(processed_ids, phase)\n",
    "                \n",
    "                # Libérer la mémoire\n",
    "                if engine.use_gpu:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "        except Exception as e:\n",
    "            with open(log_path, 'a') as log_file:\n",
    "                log_file.write(f\"ERREUR sur puzzle {puzzle_id}: {str(e)}\\n\")\n",
    "            print(f\"Erreur lors du traitement du puzzle {puzzle_id}: {str(e)}\")\n",
    "    \n",
    "    # Sauvegarde finale du point de reprise\n",
    "    save_checkpoint(processed_ids, phase)\n",
    "    \n",
    "    # Calculer les statistiques finales\n",
    "    avg_time = total_time / successful_puzzles if successful_puzzles > 0 else 0\n",
    "    \n",
    "    # Résumé des statistiques\n",
    "    summary = {\n",
    "        'total_puzzles': total_puzzles,\n",
    "        'successful_puzzles': successful_puzzles,\n",
    "        'skipped_puzzles': skipped_puzzles,\n",
    "        'failed_puzzles': total_puzzles - successful_puzzles - skipped_puzzles,\n",
    "        'avg_processing_time': avg_time,\n",
    "        'total_processing_time': total_time\n",
    "    }\n",
    "    \n",
    "    # Logging du résumé\n",
    "    with open(log_path, 'a') as log_file:\n",
    "        log_file.write(f\"\\n=== Résumé du traitement {phase} ===\\n\")\n",
    "        for key, value in summary.items():\n",
    "            log_file.write(f\"{key}: {value}\\n\")\n",
    "        log_file.write(f\"=== Fin du traitement {phase} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\\n\")\n",
    "    \n",
    "    return results, summary\n",
    "\n",
    "def convert_puzzle_format(puzzle_data):\n",
    "    \"\"\"Fonction de conversion complète pour les puzzles ARC\"\"\"\n",
    "    converted_puzzle = {}\n",
    "    \n",
    "    # Conversion de l'entrée\n",
    "    if 'train' in puzzle_data:\n",
    "        # Format complet avec train/test\n",
    "        train_data = puzzle_data['train'][0]  # Premier exemple d'entraînement\n",
    "        converted_puzzle['input'] = train_data['input']\n",
    "        converted_puzzle['output'] = train_data['output']\n",
    "    elif 'input' in puzzle_data:\n",
    "        # Format direct avec input/output\n",
    "        converted_puzzle['input'] = puzzle_data['input']\n",
    "        if 'output' in puzzle_data:\n",
    "            converted_puzzle['output'] = puzzle_data['output']\n",
    "    \n",
    "    # Ajouter des métadonnées si disponibles\n",
    "    if 'id' in puzzle_data:\n",
    "        converted_puzzle['id'] = puzzle_data['id']\n",
    "    \n",
    "    return converted_puzzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-complete-analysis",
   "metadata": {},
   "source": [
    "def run_complete_arc_analysis(engine, puzzle_data_dir=\"../input/arc-prize-2025\"):\n",
    "    \"\"\"Exécute l'analyse complète de tous les puzzles ARC sans aucune limitation\"\"\"\n",
    "    # Définir les chemins de fichiers\n",
    "    training_path = os.path.join(puzzle_data_dir, \"training\")\n",
    "    evaluation_path = os.path.join(puzzle_data_dir, \"evaluation\")\n",
    "    test_path = os.path.join(puzzle_data_dir, \"test\")\n",
    "    \n",
    "    # Charger tous les puzzles\n",
    "    training_puzzles = load_puzzles(training_path)\n",
    "    evaluation_puzzles = load_puzzles(evaluation_path)\n",
    "    test_puzzles = load_puzzles(test_path)\n",
    "    \n",
    "    print(f\"Puzzles chargés: {len(training_puzzles)} training, {len(evaluation_puzzles)} evaluation, {len(test_puzzles)} test\")\n",
    "    \n",
    "    # Traiter tous les puzzles d'entraînement\n",
    "    print(\"\\n=== Traitement des puzzles d'entraînement ===\")\n",
    "    training_results, training_summary = process_puzzles_optimized(\n",
    "        training_puzzles, engine, phase=\"training\", verify_solutions=True\n",
    "    )\n",
    "    \n",
    "    # Traiter tous les puzzles d'évaluation\n",
    "    print(\"\\n=== Traitement des puzzles d'évaluation ===\")\n",
    "    evaluation_results, evaluation_summary = process_puzzles_optimized(\n",
    "        evaluation_puzzles, engine, phase=\"evaluation\", verify_solutions=True\n",
    "    )\n",
    "    \n",
    "    # Traiter tous les puzzles de test\n",
    "    print(\"\\n=== Traitement des puzzles de test ===\")\n",
    "    test_results, test_summary = process_puzzles_optimized(\n",
    "        test_puzzles, engine, phase=\"test\", verify_solutions=False\n",
    "    )\n",
    "    \n",
    "    # Enregistrer les résultats\n",
    "    save_results(training_results, \"training_results.json\")\n",
    "    save_results(evaluation_results, \"evaluation_results.json\")\n",
    "    save_results(test_results, \"test_results.json\")\n",
    "    \n",
    "    # Enregistrer les résumés\n",
    "    save_results(training_summary, \"training_summary.json\")\n",
    "    save_results(evaluation_summary, \"evaluation_summary.json\")\n",
    "    save_results(test_summary, \"test_summary.json\")\n",
    "    \n",
    "    # Calculer la précision sur les ensembles d'entraînement et d'évaluation\n",
    "    if training_summary['successful_puzzles'] > 0 and 'solution_correct' in training_results[0]:\n",
    "        correct_count = sum(1 for r in training_results if r['solution_correct'])\n",
    "        training_accuracy = correct_count / training_summary['successful_puzzles']\n",
    "        print(f\"Précision sur l'ensemble d'entraînement: {training_accuracy:.2%}\")\n",
    "    \n",
    "    if evaluation_summary['successful_puzzles'] > 0 and 'solution_correct' in evaluation_results[0]:\n",
    "        correct_count = sum(1 for r in evaluation_results if r['solution_correct'])\n",
    "        evaluation_accuracy = correct_count / evaluation_summary['successful_puzzles']\n",
    "        print(f\"Précision sur l'ensemble d'évaluation: {evaluation_accuracy:.2%}\")\n",
    "    \n",
    "    # Résumé global\n",
    "    total_puzzles = training_summary['total_puzzles'] + evaluation_summary['total_puzzles'] + test_summary['total_puzzles']\n",
    "    successful_puzzles = training_summary['successful_puzzles'] + evaluation_summary['successful_puzzles'] + test_summary['successful_puzzles']\n",
    "    total_time = training_summary['total_processing_time'] + evaluation_summary['total_processing_time'] + test_summary['total_processing_time']\n",
    "    \n",
    "    global_summary = {\n",
    "        'total_puzzles': total_puzzles,\n",
    "        'successful_puzzles': successful_puzzles,\n",
    "        'completion_rate': successful_puzzles / total_puzzles if total_puzzles > 0 else 0,\n",
    "        'total_processing_time': total_time,\n",
    "        'avg_time_per_puzzle': total_time / successful_puzzles if successful_puzzles > 0 else 0\n",
    "    }\n",
    "    \n",
    "    save_results(global_summary, \"global_summary.json\")\n",
    "    \n",
    "    print(\"\\n=== Résumé Global ===\")\n",
    "    print(f\"Puzzles traités: {successful_puzzles}/{total_puzzles} ({global_summary['completion_rate']:.2%})\")\n",
    "    print(f\"Temps total de traitement: {total_time:.2f} secondes\")\n",
    "    print(f\"Temps moyen par puzzle: {global_summary['avg_time_per_puzzle']:.2f} secondes\")\n",
    "    \n",
    "    return global_summary\n",
    "\n",
    "def load_puzzles(directory):\n",
    "    \"\"\"Charge tous les puzzles depuis un répertoire\"\"\"\n",
    "    puzzles = []\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"ATTENTION: Répertoire {directory} introuvable\")\n",
    "        return puzzles\n",
    "    \n",
    "    # Parcourir tous les fichiers JSON dans le répertoire\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    puzzle_data = json.load(f)\n",
    "                    \n",
    "                # Ajouter l'ID du puzzle basé sur le nom de fichier\n",
    "                puzzle_id = filename.split('.')[0]\n",
    "                puzzle_data['id'] = puzzle_id\n",
    "                \n",
    "                puzzles.append(puzzle_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du chargement de {file_path}: {str(e)}\")\n",
    "    \n",
    "    return puzzles\n",
    "\n",
    "def save_results(results, filename):\n",
    "    \"\"\"Enregistre les résultats dans un fichier JSON\"\"\"\n",
    "    output_path = os.path.join(\"outputs\", filename)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-submission",
   "metadata": {},
   "source": [
    "def prepare_kaggle_submission(output_file=\"submission.json\"):\n",
    "    \"\"\"Prépare une soumission au format requis par la compétition ARC-Prize-2025\"\"\"\n",
    "    # Charger les résultats des puzzles de test\n",
    "    test_results_path = os.path.join(\"outputs\", \"test_results.json\")\n",
    "    \n",
    "    if not os.path.exists(test_results_path):\n",
    "        print(f\"ERREUR: Fichier de résultats introuvable: {test_results_path}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        with open(test_results_path, 'r') as f:\n",
    "            test_results = json.load(f)\n",
    "        \n",
    "        # Format de soumission pour la compétition\n",
    "        submission = {}\n",
    "        \n",
    "        for result in test_results:\n",
    "            puzzle_id = result['id']\n",
    "            output_grid = result['output']\n",
    "            submission[puzzle_id] = output_grid\n",
    "        \n",
    "        # Enregistrer la soumission\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(submission, f)\n",
    "        \n",
    "        print(f\"Soumission préparée avec {len(submission)} prédictions\")\n",
    "        print(f\"Fichier de soumission: {output_file}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERREUR lors de la préparation de la soumission: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surveillance-cell",
   "metadata": {},
   "source": [
    "# Cellule de surveillance du traitement Neurax3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration du logger\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "# Configurer le logger\n",
    "logger = logging.getLogger('neurax3_monitor')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Handler pour fichier\n",
    "log_file = 'logs/neurax3_monitor.log'\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Handler pour console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Formateur\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Ajouter les handlers\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Classe pour le monitoring\n",
    "class Neurax3Monitor:\n",
    "    def __init__(self):\n",
    "        self.stop_flag = threading.Event()\n",
    "        self.statistics = {\n",
    "            'training': {\n",
    "                'total': 1000,\n",
    "                'processed': 0,\n",
    "                'progress': 0.0,\n",
    "                'start_time': None,\n",
    "                'last_update': None\n",
    "            },\n",
    "            'evaluation': {\n",
    "                'total': 120,\n",
    "                'processed': 0,\n",
    "                'progress': 0.0,\n",
    "                'start_time': None,\n",
    "                'last_update': None\n",
    "            },\n",
    "            'test': {\n",
    "                'total': 240,\n",
    "                'processed': 0,\n",
    "                'progress': 0.0,\n",
    "                'start_time': None,\n",
    "                'last_update': None\n",
    "            },\n",
    "            'global': {\n",
    "                'total': 1360,\n",
    "                'processed': 0,\n",
    "                'progress': 0.0,\n",
    "                'start_time': datetime.now(),\n",
    "                'estimated_completion': None,\n",
    "                'gpu_memory_used': 0.0,\n",
    "                'cpu_memory_used': 0.0\n",
    "            }\n",
    "        }\n",
    "        self.monitoring_thread = None\n",
    "        \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Démarre le thread de surveillance\"\"\"\n",
    "        if self.monitoring_thread is None or not self.monitoring_thread.is_alive():\n",
    "            self.stop_flag.clear()\n",
    "            self.monitoring_thread = threading.Thread(target=self._monitoring_loop)\n",
    "            self.monitoring_thread.daemon = True\n",
    "            self.monitoring_thread.start()\n",
    "            logger.info(\"Surveillance démarrée\")\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Arrête le thread de surveillance\"\"\"\n",
    "        if self.monitoring_thread and self.monitoring_thread.is_alive():\n",
    "            self.stop_flag.set()\n",
    "            self.monitoring_thread.join(timeout=5)\n",
    "            logger.info(\"Surveillance arrêtée\")\n",
    "    \n",
    "    def _monitoring_loop(self):\n",
    "        \"\"\"Boucle principale de surveillance\"\"\"\n",
    "        while not self.stop_flag.is_set():\n",
    "            try:\n",
    "                self._update_statistics()\n",
    "                self._generate_progress_report()\n",
    "                self._update_status_file()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Erreur dans la boucle de surveillance: {str(e)}\")\n",
    "            \n",
    "            # Pause entre les mises à jour\n",
    "            time.sleep(60)\n",
    "    \n",
    "    def _update_statistics(self):\n",
    "        \"\"\"Met à jour les statistiques à partir des points de reprise\"\"\"\n",
    "        phases = ['training', 'evaluation', 'test']\n",
    "        \n",
    "        total_processed = 0\n",
    "        for phase in phases:\n",
    "            checkpoint_path = f\"checkpoints/{phase}_checkpoint.json\"\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                try:\n",
    "                    with open(checkpoint_path, 'r') as f:\n",
    "                        checkpoint = json.load(f)\n",
    "                    \n",
    "                    processed_count = len(checkpoint['processed_ids'])\n",
    "                    total_count = self.statistics[phase]['total']\n",
    "                    \n",
    "                    self.statistics[phase]['processed'] = processed_count\n",
    "                    self.statistics[phase]['progress'] = processed_count / total_count if total_count > 0 else 0.0\n",
    "                    self.statistics[phase]['last_update'] = checkpoint.get('timestamp', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                    \n",
    "                    if self.statistics[phase]['start_time'] is None and processed_count > 0:\n",
    "                        self.statistics[phase]['start_time'] = checkpoint.get('timestamp', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                    \n",
    "                    total_processed += processed_count\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Erreur lors de la lecture du point de reprise {phase}: {str(e)}\")\n",
    "        \n",
    "        # Mise à jour des statistiques globales\n",
    "        self.statistics['global']['processed'] = total_processed\n",
    "        self.statistics['global']['progress'] = total_processed / self.statistics['global']['total']\n",
    "        \n",
    "        # Estimation du temps restant\n",
    "        if total_processed > 0:\n",
    "            elapsed_time = (datetime.now() - self.statistics['global']['start_time']).total_seconds()\n",
    "            remaining_items = self.statistics['global']['total'] - total_processed\n",
    "            \n",
    "            if elapsed_time > 0 and remaining_items > 0:\n",
    "                time_per_item = elapsed_time / total_processed\n",
    "                remaining_time = remaining_items * time_per_item\n",
    "                estimated_completion = datetime.now() + timedelta(seconds=remaining_time)\n",
    "                self.statistics['global']['estimated_completion'] = estimated_completion.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    def _generate_progress_report(self):\n",
    "        \"\"\"Génère un rapport de progression\"\"\"\n",
    "        report_path = f\"reports/progress_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(self.statistics, f, indent=2, default=str)\n",
    "        \n",
    "        # Générer un graphique de progression\n",
    "        self._generate_progress_chart()\n",
    "    \n",
    "    def _generate_progress_chart(self):\n",
    "        \"\"\"Génère un graphique de progression\"\"\"\n",
    "        try:\n",
    "            phases = ['training', 'evaluation', 'test']\n",
    "            progress_values = [self.statistics[phase]['progress'] * 100 for phase in phases]\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            bars = plt.bar(phases, progress_values)\n",
    "            \n",
    "            # Ajouter les pourcentages sur les barres\n",
    "            for bar, value in zip(bars, progress_values):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, value + 1, f\"{value:.1f}%\", \n",
    "                         ha='center', va='bottom')\n",
    "            \n",
    "            plt.xlabel('Phase')\n",
    "            plt.ylabel('Progression (%)')\n",
    "            plt.title('Progression du traitement Neurax3')\n",
    "            plt.ylim(0, 110)  # Pour laisser de l'espace pour les étiquettes\n",
    "            \n",
    "            # Ajouter une ligne pour la progression globale\n",
    "            plt.axhline(y=self.statistics['global']['progress'] * 100, color='r', linestyle='--',\n",
    "                        label=f\"Global: {self.statistics['global']['progress'] * 100:.1f}%\")\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"reports/progress_chart_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\")\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la génération du graphique: {str(e)}\")\n",
    "    \n",
    "    def _update_status_file(self):\n",
    "        \"\"\"Met à jour le fichier de statut\"\"\"\n",
    "        status_path = \"logs/status.txt\"\n",
    "        with open(status_path, 'w') as f:\n",
    "            f.write(f\"=== STATUT NEURAX3 - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\\n\\n\")\n",
    "            \n",
    "            # Progression globale\n",
    "            global_progress = self.statistics['global']['progress'] * 100\n",
    "            f.write(f\"Progression globale: {global_progress:.1f}% ({self.statistics['global']['processed']}/{self.statistics['global']['total']})\\n\")\n",
    "            \n",
    "            # Temps estimé\n",
    "            if self.statistics['global']['estimated_completion']:\n",
    "                f.write(f\"Fin estimée: {self.statistics['global']['estimated_completion']}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Progression par phase\n",
    "            for phase in ['training', 'evaluation', 'test']:\n",
    "                stats = self.statistics[phase]\n",
    "                progress = stats['progress'] * 100\n",
    "                f.write(f\"{phase.capitalize()}: {progress:.1f}% ({stats['processed']}/{stats['total']})\")\n",
    "                \n",
    "                if stats['last_update']:\n",
    "                    f.write(f\" - Dernière mise à jour: {stats['last_update']}\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "# Démarrer le moniteur\n",
    "monitor = Neurax3Monitor()\n",
    "monitor.start_monitoring()\n",
    "\n",
    "logger.info(\"Système de surveillance et d'analyse démarré avec succès\")\n",
    "logger.info(\"Vérifiez le dossier 'logs' pour les logs détaillés\")\n",
    "logger.info(\"Vérifiez le dossier 'reports' pour les rapports d'avancement\")\n",
    "\n",
    "print(\"=== SYSTÈME DE SURVEILLANCE NEURAX3 ACTIVÉ ===\")\n",
    "print(\"Les logs et rapports seront générés automatiquement.\")\n",
    "print(\"Dossier des logs: logs/\")\n",
    "print(\"Dossier des rapports: reports/\")\n",
    "print(\"Statut actuel: logs/status.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-execution",
   "metadata": {},
   "source": [
    "# Exécution principale de l'analyse Neurax3\n",
    "from datetime import timedelta\n",
    "\n",
    "# Initialiser le moteur et configurer pour GPU\n",
    "print(\"Initialisation du moteur Neurax3...\")\n",
    "engine = QuantumGravitySimulator(use_gpu=True, precision=\"float16\", memory_efficient=True)\n",
    "\n",
    "# Configuration optimale pour GPU\n",
    "is_gpu = configure_engine_for_gpu(engine)\n",
    "print(f\"Configuration GPU réussie: {is_gpu}\")\n",
    "\n",
    "# Améliorer le moteur avec les extensions physiques\n",
    "print(\"Application des extensions physiques avancées...\")\n",
    "enhanced_engine = enhance_quantum_gravity_simulator(engine)\n",
    "print(\"Extensions physiques appliquées avec succès\")\n",
    "\n",
    "# Exécuter l'analyse complète\n",
    "print(\"\\n=== DÉMARRAGE DE L'ANALYSE COMPLÈTE ===\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "global_summary = run_complete_arc_analysis(enhanced_engine)\n",
    "\n",
    "end_time = time.time()\n",
    "total_execution_time = end_time - start_time\n",
    "hours, remainder = divmod(total_execution_time, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "print(f\"\\n=== ANALYSE TERMINÉE ===\\n\")\n",
    "print(f\"Temps d'exécution total: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "\n",
    "# Préparation de la soumission\n",
    "print(\"\\nPréparation de la soumission pour Kaggle...\")\n",
    "submission_success = prepare_kaggle_submission(\"submission.json\")\n",
    "\n",
    "if submission_success:\n",
    "    print(\"\\n=== SOUMISSION PRÊTE ===\")\n",
    "    print(\"La soumission a été préparée avec succès dans 'submission.json'\")\n",
    "else:\n",
    "    print(\"\\n=== ERREUR DE SOUMISSION ===\")\n",
    "    print(\"Impossible de préparer la soumission\")\n",
    "\n",
    "# Arrêter le système de surveillance\n",
    "if 'monitor' in globals():\n",
    "    monitor.stop_monitoring()\n",
    "    print(\"Système de surveillance arrêté\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}```python
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantum-gravity-simulator",
   "metadata": {},
   "source": [
    "class QuantumGravitySimulator:\n",
    "    \"\"\"Simulateur de gravité quantique pour les puzzles ARC\"\"\"\n",
    "    \n",
    "    def __init__(self, use_gpu=True, precision=\"float32\", memory_efficient=True):\n",
    "        self.use_gpu = use_gpu and torch.cuda.is_available()\n",
    "        self.precision = precision\n",
    "        self.memory_efficient = memory_efficient\n",
    "        self.device = torch.device(\"cuda\" if self.use_gpu else \"cpu\")\n",
    "        self.dtype = torch.float32 if precision == \"float32\" else torch.float16\n",
    "        \n",
    "        # Initialisation des paramètres quantiques\n",
    "        self.quantum_fields = {\n",
    "            \"primary\": None,\n",
    "            \"secondary\": None,\n",
    "            \"tertiary\": None,\n",
    "            \"quaternary\": None\n",
    "        }\n",
    "        self.interaction_strengths = self._initialize_interaction_strengths()\n",
    "        self.relativistic_effects = self._initialize_relativistic_effects()\n",
    "        \n",
    "        # Configurations avancées pour le GPU\n",
    "        if self.use_gpu:\n",
    "            # Activer la précision mixte pour économiser la mémoire GPU\n",
    "            self.scaler = torch.cuda.amp.GradScaler(enabled=(precision == \"float16\"))\n",
    "            # Optimiser la mémoire cache L2\n",
    "            torch.cuda.set_per_process_memory_fraction(0.95)\n",
    "            # Activer les tensor cores pour les opérations matricielles\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    \n",
    "    def _initialize_interaction_strengths(self):\n",
    "        \"\"\"Initialise les forces d'interaction entre les champs quantiques\"\"\"\n",
    "        return {\n",
    "            \"gravitational\": 1.0,\n",
    "            \"electromagnetic\": 0.7,\n",
    "            \"strong_nuclear\": 0.3,\n",
    "            \"weak_nuclear\": 0.1,\n",
    "            \"quantum_entanglement\": 0.5\n",
    "        }\n",
    "    \n",
    "    def _initialize_relativistic_effects(self):\n",
    "        \"\"\"Initialise les effets relativistes dans la simulation\"\"\"\n",
    "        return {\n",
    "            \"time_dilation\": 0.2,\n",
    "            \"length_contraction\": 0.1,\n",
    "            \"mass_energy_equivalence\": 0.3,\n",
    "            \"quantum_tunneling\": 0.4\n",
    "        }\n",
    "    \n",
    "    def initialize_fields(self, grid_size):\n",
    "        \"\"\"Initialise les champs quantiques pour une taille de grille donnée\"\"\"\n",
    "        # Création des champs quantiques sur le dispositif approprié\n",
    "        for field_name in self.quantum_fields:\n",
    "            self.quantum_fields[field_name] = torch.zeros(\n",
    "                (grid_size, grid_size), \n",
    "                dtype=self.dtype, \n",
    "                device=self.device\n",
    "            )\n",
    "    \n",
    "    def apply_quantum_gravity(self, grid, iterations=5):\n",
    "        \"\"\"Applique les effets de gravité quantique sur une grille\"\"\"\n",
    "        # Conversion de la grille en tenseur PyTorch\n",
    "        if not isinstance(grid, torch.Tensor):\n",
    "            grid = torch.tensor(grid, dtype=self.dtype, device=self.device)\n",
    "        elif grid.device != self.device or grid.dtype != self.dtype:\n",
    "            grid = grid.to(device=self.device, dtype=self.dtype)\n",
    "            \n",
    "        # Initialisation des champs si nécessaire\n",
    "        if self.quantum_fields[\"primary\"] is None or self.quantum_fields[\"primary\"].shape != grid.shape:\n",
    "            self.initialize_fields(grid.shape[0])\n",
    "        \n",
    "        # Application des effets quantiques gravitationnels\n",
    "        with torch.cuda.amp.autocast(enabled=(self.precision == \"float16\")):\n",
    "            # Mise à jour du champ primaire\n",
    "            self.quantum_fields[\"primary\"] = grid.clone()\n",
    "            \n",
    "            # Calcul des champs secondaires\n",
    "            for i in range(iterations):\n",
    "                # Interactions gravitationnelles\n",
    "                gravitational_field = self._compute_gravitational_field(self.quantum_fields[\"primary\"])\n",
    "                self.quantum_fields[\"secondary\"] = gravitational_field\n",
    "                \n",
    "                # Interactions électromagnétiques\n",
    "                electromagnetic_field = self._compute_electromagnetic_field(self.quantum_fields[\"primary\"])\n",
    "                self.quantum_fields[\"tertiary\"] = electromagnetic_field\n",
    "                \n",
    "                # Effets quantiques et relativistes\n",
    "                quantum_field = self._apply_quantum_effects(self.quantum_fields[\"primary\"], \n",
    "                                                        self.quantum_fields[\"secondary\"],\n",
    "                                                        self.quantum_fields[\"tertiary\"])\n",
    "                self.quantum_fields[\"quaternary\"] = quantum_field\n",
    "                \n",
    "                # Combinaison des champs pour l'itération suivante\n",
    "                combined_field = self._combine_fields()\n",
    "                self.quantum_fields[\"primary\"] = combined_field\n",
    "        \n",
    "        # Conversion finale et nettoyage de la mémoire si nécessaire\n",
    "        result = self.quantum_fields[\"primary\"].clone()\n",
    "        if self.memory_efficient and self.use_gpu:\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def _compute_gravitational_field(self, field):\n",
    "        \"\"\"Calcule le champ gravitationnel à partir d'un champ donné\"\"\"\n",
    "        # Convolution pour simuler les effets gravitationnels\n",
    "        kernel = torch.tensor([[0.5, 1.0, 0.5],\n",
    "                              [1.0, 0.0, 1.0],\n",
    "                              [0.5, 1.0, 0.5]], dtype=self.dtype, device=self.device)\n",
    "        kernel = kernel.view(1, 1, 3, 3)\n",
    "        \n",
    "        # Préparer le champ pour la convolution\n",
    "        field_reshaped = field.view(1, 1, field.shape[0], field.shape[1])\n",
    "        \n",
    "        # Appliquer la convolution\n",
    "        padding = 1\n",
    "        gravitational_field = torch.nn.functional.conv2d(field_reshaped, kernel, padding=padding)\n",
    "        \n",
    "        # Normaliser et appliquer la force gravitationnelle\n",
    "        gravitational_field = gravitational_field.view(field.shape)\n",
    "        gravitational_field = gravitational_field * self.interaction_strengths[\"gravitational\"]\n",
    "        \n",
    "        return gravitational_field\n",
    "    \n",
    "    def _compute_electromagnetic_field(self, field):\n",
    "        \"\"\"Calcule le champ électromagnétique à partir d'un champ donné\"\"\"\n",
    "        # Détecter les gradients pour simuler les effets électromagnétiques\n",
    "        sobel_x = torch.tensor([[-1, 0, 1],\n",
    "                              [-2, 0, 2],\n",
    "                              [-1, 0, 1]], dtype=self.dtype, device=self.device)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1],\n",
    "                              [0, 0, 0],\n",
    "                              [1, 2, 1]], dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        sobel_x = sobel_x.view(1, 1, 3, 3)\n",
    "        sobel_y = sobel_y.view(1, 1, 3, 3)\n",
    "        \n",
    "        # Préparer le champ pour la convolution\n",
    "        field_reshaped = field.view(1, 1, field.shape[0], field.shape[1])\n",
    "        \n",
    "        # Appliquer les filtres Sobel\n",
    "        padding = 1\n",
    "        grad_x = torch.nn.functional.conv2d(field_reshaped, sobel_x, padding=padding)\n",
    "        grad_y = torch.nn.functional.conv2d(field_reshaped, sobel_y, padding=padding)\n",
    "        \n",
    "        # Calculer la magnitude du gradient\n",
    "        electromagnetic_field = torch.sqrt(grad_x**2 + grad_y**2 + 1e-6)\n",
    "        electromagnetic_field = electromagnetic_field.view(field.shape)\n",
    "        \n",
    "        # Appliquer la force électromagnétique\n",
    "        electromagnetic_field = electromagnetic_field * self.interaction_strengths[\"electromagnetic\"]\n",
    "        \n",
    "        return electromagnetic_field\n",
    "    \n",
    "    def _apply_quantum_effects(self, primary_field, secondary_field, tertiary_field):\n",
    "        \"\"\"Applique les effets quantiques sur les champs\"\"\"\n",
    "        # Entanglement quantique entre les champs\n",
    "        entanglement = (primary_field * secondary_field * self.interaction_strengths[\"quantum_entanglement\"])\n",
    "        \n",
    "        # Effets tunnels quantiques\n",
    "        tunneling = torch.sigmoid(tertiary_field) * self.relativistic_effects[\"quantum_tunneling\"]\n",
    "        \n",
    "        # Combinaison des effets quantiques\n",
    "        quantum_field = primary_field + entanglement + tunneling\n",
    "        \n",
    "        return quantum_field\n",
    "    \n",
    "    def _combine_fields(self):\n",
    "        \"\"\"Combine tous les champs quantiques pour produire le champ résultant\"\"\"\n",
    "        # Moyenne pondérée des champs\n",
    "        weights = torch.tensor([0.4, 0.3, 0.2, 0.1], dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        combined_field = (\n",
    "            weights[0] * self.quantum_fields[\"primary\"] +\n",
    "            weights[1] * self.quantum_fields[\"secondary\"] +\n",
    "            weights[2] * self.quantum_fields[\"tertiary\"] +\n",
    "            weights[3] * self.quantum_fields[\"quaternary\"]\n",
    "        )\n",
    "        \n",
    "        # Normalisation des valeurs\n",
    "        combined_field = torch.clamp(combined_field, 0, 10)\n",
    "        \n",
    "        return combined_field\n",
    "    \n",
    "    def process_grid(self, grid, time_steps=5):\n",
    "        \"\"\"Traite une grille de puzzle ARC avec des effets gravitationnels quantiques\"\"\"\n",
    "        # Conversion initiale de la grille\n",
    "        if not isinstance(grid, torch.Tensor):\n",
    "            processed_grid = torch.tensor(grid, dtype=self.dtype, device=self.device)\n",
    "        else:\n",
    "            processed_grid = grid.clone().to(device=self.device, dtype=self.dtype)\n",
    "            \n",
    "        # Application de la gravité quantique pour chaque étape temporelle\n",
    "        for _ in range(time_steps):\n",
    "            processed_grid = self.apply_quantum_gravity(processed_grid)\n",
    "            \n",
    "        # Conversion finale vers le format entier pour ARC\n",
    "        result = torch.round(processed_grid).to(torch.int64)\n",
    "        \n",
    "        # Conversion en numpy pour compatibilité avec ARC\n",
    "        if self.use_gpu:\n",
    "            result = result.cpu().numpy()\n",
    "        else:\n",
    "            result = result.numpy()\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-quantum-simulator",
   "metadata": {},
   "source": [
    "def enhance_quantum_gravity_simulator(engine):\n",
    "    \"\"\"Implémente les extensions physiques avancées pour le simulateur de gravité quantique.\"\"\"\n",
    "    # Ajouter les champs quantiques supplémentaires\n",
    "    engine.quantum_fields[\"quintessence\"] = None\n",
    "    engine.quantum_fields[\"dark_matter\"] = None\n",
    "    engine.quantum_fields[\"dark_energy\"] = None\n",
    "    \n",
    "    # Augmenter les forces d'interaction\n",
    "    engine.interaction_strengths[\"quantum_entanglement\"] = 0.75  # Augmenter l'intrication quantique\n",
    "    engine.interaction_strengths[\"non_local\"] = 0.6  # Ajouter les interactions non-locales\n",
    "    engine.interaction_strengths[\"dimensional\"] = 0.4  # Ajouter les interactions dimensionnelles\n",
    "    \n",
    "    # Améliorer les effets relativistes\n",
    "    engine.relativistic_effects[\"time_dilation\"] = 0.3  # Augmenter les effets de dilatation temporelle\n",
    "    engine.relativistic_effects[\"quantum_tunneling\"] = 0.6  # Augmenter les effets de tunnelisation\n",
    "    engine.relativistic_effects[\"superposition\"] = 0.5  # Ajouter la superposition quantique\n",
    "    \n",
    "    # Extension pour les algorithmes adaptatifs\n",
    "    original_initialize_fields = engine.initialize_fields\n",
    "    \n",
    "    def enhanced_initialize_fields(grid_size):\n",
    "        # Appel à la méthode originale\n",
    "        original_initialize_fields(grid_size)\n",
    "        \n",
    "        # Initialisation des champs supplémentaires\n",
    "        engine.quantum_fields[\"quintessence\"] = torch.zeros(\n",
    "            (grid_size, grid_size), \n",
    "            dtype=engine.dtype, \n",
    "            device=engine.device\n",
    "        )\n",
    "        engine.quantum_fields[\"dark_matter\"] = torch.zeros(\n",
    "            (grid_size, grid_size), \n",
    "            dtype=engine.dtype, \n",
    "            device=engine.device\n",
    "        )\n",
    "        engine.quantum_fields[\"dark_energy\"] = torch.zeros(\n",
    "            (grid_size, grid_size), \n",
    "            dtype=engine.dtype, \n",
    "            device=engine.device\n",
    "        )\n",
    "    \n",
    "    # Remplacer la méthode d'initialisation\n",
    "    engine.initialize_fields = enhanced_initialize_fields\n",
    "    \n",
    "    # Extension pour la compression des états quantiques\n",
    "    original_combine_fields = engine._combine_fields\n",
    "    \n",
    "    def enhanced_combine_fields():\n",
    "        # Obtenir le champ combiné de base\n",
    "        combined_field = original_combine_fields()\n",
    "        \n",
    "        # Ajouter l'influence des champs supplémentaires s'ils existent\n",
    "        if engine.quantum_fields[\"quintessence\"] is not None:\n",
    "            # Effet de quintessence (influencé par les motifs globaux)\n",
    "            quintessence_weight = 0.15\n",
    "            combined_field = combined_field + quintessence_weight * engine.quantum_fields[\"quintessence\"]\n",
    "        \n",
    "        if engine.quantum_fields[\"dark_matter\"] is not None:\n",
    "            # Effet de matière noire (augmente la \"masse\" des motifs)\n",
    "            dark_matter_weight = 0.2\n",
    "            combined_field = combined_field + dark_matter_weight * engine.quantum_fields[\"dark_matter\"]\n",
    "            \n",
    "        if engine.quantum_fields[\"dark_energy\"] is not None:\n",
    "            # Effet d'énergie noire (expansion des motifs)\n",
    "            dark_energy_weight = 0.1\n",
    "            combined_field = combined_field + dark_energy_weight * engine.quantum_fields[\"dark_energy\"]\n",
    "        \n",
    "        # Compression des états quantiques (réduire la dispersion des valeurs)\n",
    "        mean = torch.mean(combined_field)\n",
    "        std = torch.std(combined_field) + 1e-6\n",
    "        normalized_field = (combined_field - mean) / std\n",
    "        compressed_field = torch.tanh(normalized_field) * 5 + 5  # Remappe dans la plage 0-10\n",
    "        \n",
    "        # Fusion contrôlée entre le champ original et le champ compressé\n",
    "        compression_ratio = 0.3\n",
    "        final_field = (1 - compression_ratio) * combined_field + compression_ratio * compressed_field\n",
    "        \n",
    "        # Garantir que les valeurs restent dans la plage attendue\n",
    "        final_field = torch.clamp(final_field, 0, 10)\n",
    "        \n",
    "        return final_field\n",
    "    \n",
    "    # Remplacer la méthode de combinaison\n",
    "    engine._combine_fields = enhanced_combine_fields\n",
    "    \n",
    "    return engine  # Retourner le moteur amélioré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configure-gpu-section",
   "metadata": {},
   "source": [
    "def configure_engine_for_gpu(engine):\n",
    "    \"\"\"Configure le moteur Neurax pour utiliser le GPU de manière optimale\"\"\"\n",
    "    # Vérifier si un GPU est disponible\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"ATTENTION: Aucun GPU détecté. Utilisation du CPU.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Configurer pour utiliser le GPU avec des options avancées\n",
    "        print(f\"Configuration GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        # Optimisations CUDA\n",
    "        torch.backends.cudnn.benchmark = True  # Optimiser les performances pour des tailles d'entrée constantes\n",
    "        torch.backends.cudnn.deterministic = False  # Permettre des optimisations non-déterministes\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True  # Utiliser les Tensor Cores si disponibles\n",
    "        \n",
    "        # Optimiser l'allocation mémoire\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Vérifier la mémoire GPU disponible\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "        gpu_memory_allocated = torch.cuda.memory_allocated(0)\n",
    "        gpu_memory_free = gpu_memory - gpu_memory_allocated\n",
    "        \n",
    "        print(f\"Mémoire GPU totale: {gpu_memory/1024**3:.2f} GB\")\n",
    "        print(f\"Mémoire GPU disponible: {gpu_memory_free/1024**3:.2f} GB\")\n",
    "        \n",
    "        # Configurer la précision selon la mémoire disponible\n",
    "        if gpu_memory_free > 8 * 1024**3:\n",
    "            print(\"Utilisation de la précision float32 pour une meilleure précision\")\n",
    "            engine.precision = \"float32\"\n",
    "            engine.dtype = torch.float32\n",
    "        else:\n",
    "            print(\"Utilisation de la précision mixte (float16) pour économiser la mémoire\")\n",
    "            engine.precision = \"float16\"\n",
    "            engine.dtype = torch.float16\n",
    "            engine.scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "        \n",
    "        # Vérifier si les champs quantiques sont déjà sur GPU\n",
    "        for field_name, field in engine.quantum_fields.items():\n",
    "            if field is not None and field.device.type != \"cuda\":\n",
    "                engine.quantum_fields[field_name] = field.to(device=\"cuda\", dtype=engine.dtype)\n",
    "        \n",
    "        # Nettoyer la mémoire GPU\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERREUR lors de la configuration du GPU: {str(e)}\")\n",
    "        print(\"Retour au mode CPU\")\n",
    "        engine.use_gpu = False\n",
    "        engine.device = torch.device(\"cpu\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-management-section",
   "metadata": {},
   "source": [
    "def save_checkpoint(processed_ids, phase):\n",
    "    \"\"\"Sauvegarde un point de reprise pour le traitement des puzzles\"\"\"\n",
    "    checkpoint_path = f\"checkpoints/{phase}_checkpoint.json\"\n",
    "    with open(checkpoint_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'processed_ids': processed_ids,\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, f)\n",
    "    print(f\"Point de reprise sauvegardé: {len(processed_ids)} puzzles traités\")\n",
    "    \n",
    "def load_checkpoint(phase):\n",
    "    \"\"\"Charge un point de reprise existant\"\"\"\n",
    "    checkpoint_path = f\"checkpoints/{phase}_checkpoint.json\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "        print(f\"Point de reprise chargé: {len(checkpoint['processed_ids'])} puzzles déjà traités\")\n",
    "        return checkpoint['processed_ids']\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-puzzles-section",
   "metadata": {},
   "source": [
    "def process_puzzles_optimized(puzzles, engine, max_time_per_puzzle=None, phase=\"test\", verify_solutions=False):\n",
    "    \"\"\"Version optimisée de process_puzzles sans aucune limitation.\"\"\"\n",
    "    results = []\n",
    "    total_puzzles = len(puzzles)\n",
    "    processed_ids = load_checkpoint(phase)\n",
    "    processed_set = set(processed_ids)\n",
    "    \n",
    "    # Statistiques\n",
    "    total_time = 0\n",
    "    successful_puzzles = 0\n",
    "    skipped_puzzles = 0\n",
    "    \n",
    "    # Logging\n",
    "    log_path = f\"logs/{phase}_processing.log\"\n",
    "    with open(log_path, 'a') as log_file:\n",
    "        log_file.write(f\"\\n=== Début du traitement {phase} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\\n\")\n",
    "        log_file.write(f\"Total de puzzles à traiter: {total_puzzles}\\n\")\n",
    "        log_file.write(f\"Puzzles déjà traités: {len(processed_ids)}\\n\")\n",
    "    \n",
    "    print(f\"Traitement de {total_puzzles} puzzles ({phase})...\")\n",
    "    \n",
    "    # AUCUNE limitation sur le nombre de puzzles - traitement absolument complet\n",
    "    for i, puzzle in enumerate(tqdm(puzzles, desc=f\"Traitement {phase}\")):\n",
    "        puzzle_id = puzzle.get('id', f\"{phase}_{i}\")\n",
    "        \n",
    "        # Sauter les puzzles déjà traités\n",
    "        if puzzle_id in processed_set:\n",
    "            skipped_puzzles += 1\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Conversion du format de puzzle\n",
    "            converted_puzzle = convert_puzzle_format(puzzle)\n",
    "            \n",
    "            # Traitement du puzzle avec le moteur quantique\n",
    "            input_grid = np.array(converted_puzzle['input'])\n",
    "            processed_grid = engine.process_grid(input_grid, time_steps=8)\n",
    "            \n",
    "            # Assurer que la sortie est de type int\n",
    "            output_grid = processed_grid.astype(int).tolist()\n",
    "            \n",
    "            # Calculer le temps de traitement\n",
    "            processing_time = time.time() - start_time\n",
    "            total_time += processing_time\n",
    "            \n",
    "            # Vérification des solutions (si demandé et disponible)\n",
    "            solution_correct = None\n",
    "            if verify_solutions and 'output' in converted_puzzle:\n",
    "                expected_output = np.array(converted_puzzle['output'])\n",
    "                predicted_output = np.array(output_grid)\n",
    "                solution_correct = np.array_equal(expected_output, predicted_output)\n",
    "            \n",
    "            # Enregistrer le résultat\n",
    "            result = {\n",
    "                'id': puzzle_id,\n",
    "                'output': output_grid,\n",
    "                'processing_time': processing_time,\n",
    "                'solution_correct': solution_correct\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Mettre à jour les statistiques\n",
    "            successful_puzzles += 1\n",
    "            \n",
    "            # Ajouter à la liste des puzzles traités\n",
    "            processed_ids.append(puzzle_id)\n",
    "            processed_set.add(puzzle_id)\n",
    "            \n",
    "            # Logging détaillé\n",
    "            with open(log_path, 'a') as log_file:\n",
    "                log_file.write(f\"Puzzle {puzzle_id} traité en {processing_time:.2f}s\")\n",
    "                if solution_correct is not None:\n",
    "                    log_file.write(f\" - Solution correcte: {solution_correct}\")\n",
    "                log_file.write(\"\\n\")\n",
    "            \n",
    "            # Sauvegarde périodique des points de reprise (tous les 10 puzzles)\n",
    "            if len(results) % 10 == 0:\n",
    "                save_checkpoint(processed_ids, phase)\n",
    "                \n",
    "                # Libérer la mémoire\n",
    "                if engine.use_gpu:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "        except Exception as e:\n",
    "            with open(log_path, 'a') as log_file:\n",
    "                log_file.write(f\"ERREUR sur puzzle {puzzle_id}: {str(e)}\\n\")\n",
    "            print(f\"Erreur lors du traitement du puzzle {puzzle_id}: {str(e)}\")\n",
    "    \n",
    "    # Sauvegarde finale du point de reprise\n",
    "    save_checkpoint(processed_ids, phase)\n",
    "    \n",
    "    # Calculer les statistiques finales\n",
    "    avg_time = total_time / successful_puzzles if successful_puzzles > 0 else 0\n",
    "    \n",
    "    # Résumé des statistiques\n",
    "    summary = {\n",
    "        'total_puzzles': total_puzzles,\n",
    "        'successful_puzzles': successful_puzzles,\n",
    "        'skipped_puzzles': skipped_puzzles,\n",
    "        'failed_puzzles': total_puzzles - successful_puzzles - skipped_puzzles,\n",
    "        'avg_processing_time': avg_time,\n",
    "        'total_processing_time': total_time\n",
    "    }\n",
    "    \n",
    "    # Logging du résumé\n",
    "    with open(log_path, 'a') as log_file:\n",
    "        log_file.write(f\"\\n=== Résumé du traitement {phase} ===\\n\")\n",
    "        for key, value in summary.items():\n",
    "            log_file.write(f\"{key}: {value}\\n\")\n",
    "        log_file.write(f\"=== Fin du traitement {phase} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\\n\")\n",
    "    \n",
    "    return results, summary\n",
    "\n",
    "def convert_puzzle_format(puzzle_data):\n",